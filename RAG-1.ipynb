{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1078, which is longer than the specified 180\n",
      "Created a chunk of size 1281, which is longer than the specified 180\n",
      "Created a chunk of size 375, which is longer than the specified 180\n",
      "Created a chunk of size 221, which is longer than the specified 180\n",
      "Created a chunk of size 425, which is longer than the specified 180\n",
      "Created a chunk of size 1257, which is longer than the specified 180\n",
      "Created a chunk of size 661, which is longer than the specified 180\n",
      "Created a chunk of size 355, which is longer than the specified 180\n",
      "Created a chunk of size 361, which is longer than the specified 180\n",
      "Created a chunk of size 425, which is longer than the specified 180\n",
      "Created a chunk of size 1257, which is longer than the specified 180\n",
      "Created a chunk of size 661, which is longer than the specified 180\n",
      "Created a chunk of size 323, which is longer than the specified 180\n",
      "Created a chunk of size 347, which is longer than the specified 180\n",
      "Created a chunk of size 198, which is longer than the specified 180\n",
      "Created a chunk of size 317, which is longer than the specified 180\n",
      "Created a chunk of size 224, which is longer than the specified 180\n",
      "Created a chunk of size 361, which is longer than the specified 180\n",
      "Created a chunk of size 300, which is longer than the specified 180\n",
      "Created a chunk of size 360, which is longer than the specified 180\n",
      "Created a chunk of size 246, which is longer than the specified 180\n",
      "Created a chunk of size 372, which is longer than the specified 180\n",
      "Created a chunk of size 342, which is longer than the specified 180\n",
      "Created a chunk of size 346, which is longer than the specified 180\n",
      "Created a chunk of size 376, which is longer than the specified 180\n",
      "Created a chunk of size 284, which is longer than the specified 180\n",
      "Created a chunk of size 302, which is longer than the specified 180\n",
      "Created a chunk of size 292, which is longer than the specified 180\n",
      "Created a chunk of size 427, which is longer than the specified 180\n",
      "Created a chunk of size 410, which is longer than the specified 180\n",
      "Created a chunk of size 212, which is longer than the specified 180\n",
      "Created a chunk of size 233, which is longer than the specified 180\n",
      "Created a chunk of size 370, which is longer than the specified 180\n",
      "Created a chunk of size 342, which is longer than the specified 180\n",
      "Created a chunk of size 344, which is longer than the specified 180\n",
      "Created a chunk of size 376, which is longer than the specified 180\n",
      "Created a chunk of size 272, which is longer than the specified 180\n",
      "Created a chunk of size 300, which is longer than the specified 180\n",
      "Created a chunk of size 292, which is longer than the specified 180\n",
      "Created a chunk of size 425, which is longer than the specified 180\n",
      "Created a chunk of size 410, which is longer than the specified 180\n",
      "Created a chunk of size 337, which is longer than the specified 180\n",
      "Created a chunk of size 336, which is longer than the specified 180\n",
      "Created a chunk of size 198, which is longer than the specified 180\n",
      "Created a chunk of size 198, which is longer than the specified 180\n",
      "Created a chunk of size 270, which is longer than the specified 180\n",
      "Created a chunk of size 331, which is longer than the specified 180\n",
      "Created a chunk of size 425, which is longer than the specified 180\n",
      "Created a chunk of size 198, which is longer than the specified 180\n",
      "Created a chunk of size 292, which is longer than the specified 180\n",
      "Created a chunk of size 292, which is longer than the specified 180\n",
      "/var/folders/5x/4fplq2695w37pgz057mppyhh0000gn/T/ipykernel_22964/3767789737.py:124: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests, copy\n",
    "import tiktoken\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader   #  텍스트 로더 : For \"This model's maximum context length is 16385 tokens\" error\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "RAW_URL = \"https://gist.githubusercontent.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223/raw\"\n",
    "files_dir = Path(\"./files\")\n",
    "files_dir.mkdir(parents=True, exist_ok=True)\n",
    "doc_path = files_dir / \"chapter_1.txt\"\n",
    "\n",
    "if not doc_path.exists():\n",
    "    r = requests.get(RAW_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    doc_path.write_text(r.text, encoding=\"utf-8\")\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, max_tokens=512)\n",
    "\n",
    "\n",
    "loader = TextLoader(str(doc_path), encoding=\"utf-8\")\n",
    "raw_docs = loader.load()\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=180,   \n",
    "    chunk_overlap=30,\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "emb = OpenAIEmbeddings()\n",
    "cached_emb = CacheBackedEmbeddings.from_bytes_store(emb, cache_dir)\n",
    "vs = FAISS.from_documents(docs, cached_emb)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 1})   # ✅ 한 번에 1개 청크만\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"question\",   \n",
    "    output_key=\"answer\",    \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer only using the context below.\\n\"\n",
    "            \"If you don't know the answer, say you don't know.\\n\\n\"\n",
    "            \"Context:\\n{context}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    "    output_key=\"answer\",\n",
    ")\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    ")\n",
    "\n",
    "model = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def shrink_docs_to_tokens(docs, budget_tokens=2200):\n",
    "    out, used = [], 0\n",
    "    for d in docs:\n",
    "        toks = model.encode(d.page_content)\n",
    "        n = len(toks)\n",
    "        if used + n <= budget_tokens:\n",
    "            out.append(d)\n",
    "            used += n\n",
    "        else:\n",
    "            remain = budget_tokens - used\n",
    "            if remain > 0:\n",
    "                d2 = copy.deepcopy(d)\n",
    "                d2.page_content = model.decode(toks[:remain])\n",
    "                out.append(d2)\n",
    "            break\n",
    "    return out\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "def ask(question: str) -> str:\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return qa_chain.run(input_documents=docs, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ce5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Aaronson 은 유죄인가요?\n",
      "A: 예, Jones, Aaronson, 그리고 Rutherford은 범죄를 저질렀다고 여겨졌습니다. 그러나 문서에서 Aaronson이 유죄로 판결받았는지에 대한 구체적인 정보는 제공되지 않았습니다.\n",
      "Q: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "A: 해당 문서에서는 그가 테이블에 어떤 메시지를 썼는지에 대한 정보가 제공되지 않습니다.\n",
      "Q: Julia 는 누구인가요?\n",
      "A: 'Julia'는 문맥에서 명확히 언급되지 않았기 때문에 정확한 정보를 제공할 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    qs = [\n",
    "        \"Aaronson 은 유죄인가요?\",\n",
    "        \"그가 테이블에 어떤 메시지를 썼나요?\",\n",
    "        \"Julia 는 누구인가요?\",\n",
    "    ]\n",
    "    for q in qs:\n",
    "        print(\"Q:\", q)\n",
    "        print(\"A:\", ask(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
